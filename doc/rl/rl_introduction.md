# 1. 强化学习概述

## 1.1 什么是强化学习

强化学习（reinforcement learning，RL）讨论的问题是智能体（agent）怎么在复杂、不确定的环境（environment）中最大化它能获得的奖励。如图所示，强化学习由两部分组成：智能体和环境。智能体的目的就是尽可能多地从环境中获取奖励。

## 1.2 强化学习、深度学习、深度强化学习、监督学习、非监督学习、机器学习关联与区别

## 1.3 相关名词

- 动作（action）：环境接收到的智能体基于当前状态的输出。
- 状态（state）：智能体从环境中获取的状态。
- 奖励（reward）：智能体从环境中获取的反馈信号，这个信号指定了智能体在某一步采取了某个策略以后是否得到奖励，以及奖励的大小。
- 探索（exploration）：在当前的情况下，继续尝试新的动作。其有可能得到更高的奖励，也有可能一无所有。
- 开发（exploitation）：在当前的情况下，继续尝试已知的可以获得最大奖励的过程，即选择重复执行当前动作。
- 动作空间（action space）、离散动作空间（discrete action space）和连续动作空间（continuous action space）：在给定的环境中，有效动作的集合被称为动作空间，智能体的动作数量有限的动作空间称为离散动作空间，反之，则被称为连续动作空间。
- 全部可观测（full observability）：当智能体的状态与环境的状态等价时，我们就称这个环境是全部可观测的。
- 完全可观测（fully observed）：当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的。
- 部分可观测（partially observed）：一般智能体不能观察到环境的所有状态时，我们称这个环境是部分可观测的。
- 基于策略的（policy-based）：智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够带来最大的奖励。
- 基于价值的（valued-based）：智能体不需要制定显式的策略，它维护一个价值表格或者价值函数，并通过这个价值表格或价值函数来选取价值最大化的动作。
- 有模型（model-based）结构：智能体通过学习状态的转移来进行决策。
- 免模型（model-free）结构：智能体没有直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数或者策略函数进行决策。
- rollout：指的是在训练过程中，智能体根据当前的策略在环境中进行一系列的交互步骤，模拟并收集样本数据的过程。 在每个rollout中，智能体从环境中观测当前状态，然后根据选择的策略采取一个动作。接下来，智能体与环境进行交互，执行该动作，并观察到下一个状态和获得的奖励。此后，智能体根据新的状态更新其策略，并在下一个步骤中选择下一个动作。这个过程会持续进行，直到满足某个停止条件，比如达到最大步数或达到终止状态。 Rollout的目的是通过与环境的交互来生成样本数据，用于策略优化和价值函数估计。这些样本数据将被用于更新策略参数或进行价值函数的拟合，以改善智能体的性能。通常，rollout会在训练过程中进行多次，以收集足够的样本数据来训练和优化智能体。 Rollout的长度可以根据具体任务进行调整，可以是固定长度，也可以是变长的。在一些连续控制任务中，rollout可能会持续进行数百或数千个时间步。然而，在一些离散决策任务中，rollout可能会在有限步数内完成。 总而言之，rollout是强化学习中一种通过与环境进行交互并收集样本数据的过程，用于训练智能体的策略和价值函数。

# 2. 强化学习基本框架

马尔可夫决策过程是强化学习的基本框架。
马尔可夫过程→马尔可夫奖励过程→马尔可夫决策过程
## 2.1 马尔可夫过程

马尔可夫过程（Markov process，MP）是一组具有马尔可夫性质的随机变量序列 $s_1,···,s_t$ ，其中下一个时刻的状态 $$s_{t+1}$$ 只取决于当前状态 $$s_t$$ 。我们设状态的历史为 $$h_t=\{s_1,s_2,s_3,···,s_t\}$$ ( $$h_t$$ 包含了之前的所有状态)，则马尔可夫过程满足条件：
$$p(s_{t+1}|s_t)=p(s_{t+1}|h_t)$$
从当前状态 $$s_t$$ 转移到 $$s_{t+1}$$ 就等于它之前所有状态转移到 $$s_{t+1}$$ 。
### 2.1.1 马尔可夫性质

在随机过程中，马尔可夫性质（Markov property）是指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。马尔可夫性质是所有马尔可夫过程的基础。
- 离散随机过程例子
假设随机变量 $$X_0,X_1,···,X_T$$ 构成一个随机过程。这些随机变量的所有可能取值的集合被称为状态空间（state space）。如果 $$X_{T+1}$$ 对于过去状态的条件概率分布仅是 $$X_T$$ 的一个函数，则
$$p(X_{t+1}=x_{t+1}|X_{0:t}=x_{0:t})=p(X_{t+1}=x_{t+1}|X_{t}=x_t)$$
其中， $$X_{0:t}$$ 表示变量集合 $$X_0,X_1,···,X_t$$ ， $$x_{0:t}$$ 为在状态空间中的状态序列 $$x_0,x_1,···,x_t$$ 。
### 2.1.2 马尔可夫链

离散时间的马尔可夫过程也称为马尔可夫链（Markov chain）。可以用状态转移矩阵（state transition matrix） $$\mathbf{P}$$ 来描述状态转移 $$p(s_{t+1}=s^{\prime}|s_t=s)$$ 。

$$
\mathbf{P}=\begin{pmatrix}p(s_1|s_1)&p(s_2|s_1)&···&p(s_N|s_1)\\
p(s_1|s_2)&p(s_2|s_2)&···&p(s_N|s_2)\\
\vdots & \vdots && \vdots \\
p(s_1|s_N)&p(s_2|s_N)&···&p(s_N|s_N)\end{pmatrix}
$$

- 例子
如图所示有4个状态，这4个状态在 $$s_1、s_2、s_3、s_4$$ 之间相互转移。从 $$s_1$$ 开始， $$s_1$$ 有 0.1 的概率继续存留在 $$s_1$$ 状态，有 0.2 的概率转移到 $$s_2$$ ，有 0.7 的概率转移到 $$s_4$$ 。如果 $$s_4$$ 是我们的当前状态，它有 0.3 的概率转移到 $$s_2$$ ，有 0.2 的概率转移到 $$s_3$$ ，有 0.5 的概率留在当前状态。

## 2.2 马尔可夫奖励过程

马尔可夫奖励过程（Markov reward process, MRP）是马尔可夫链加上奖励函数。在马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了奖励函数（reward function）。
### 2.2.1 回报和价值函数

范围（horizon）是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的。 回报（return）是指把奖励进行折扣后所获得的奖励。回报可以定义为奖励的逐步叠加，即

$$
G_t=r_{t+1}+{\gamma}r_{t+2}+{\gamma}^2r_{t+3}+{\gamma}^3r_{t+4}+···+{\gamma}^{T-t-1}r_T
$$

这里有一个折扣因子 $$\gamma$$ ，对未来的奖励要打折扣。用回报定义状态的价值，就是状态价值函数（state-value function）。对于马尔可夫奖励过程，状态价值函数被定义成回报的期望，即

$$\begin{align}
V^t(s)&=E[G_t|s_t=s] \nonumber\\
&=E[r_{t+1}+{\gamma}r_{t+2}+{\gamma}^2r_{t+3}+{\gamma}^3r_{t+4}+···+{\gamma}^{T-t-1}r_T|s_t=s]\nonumber
\end{align}$$

使用折扣因子的原因：
1. 有些马尔可夫过程是带环的，它并不会终结，我们想避免无穷的奖励。
2. 不能建立完美的模拟环境的模型，对未来的评估不一定是准确的，不一定完全信任模型，因为这种不确定性，所以对未来的评估增加一个折扣。把不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个时刻得到奖励。
3. 如果奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。
4. 有些时候可以把折扣因子设为0，我们就只关注当前的奖励。也可以把折扣因子设为1，对未来的奖励并没有打折扣，未来获得的奖励与当前获得的奖励是一样的。折扣因子可以作为强化学习智能体的一个超参数（hyperparameter）来进行调整，通过调整折扣因子，我们可以得到不同动作的智能体。

- 例子
如下图马尔可夫奖励过程，其奖励函数为 $$\mathbf{R}=[5,0,0,0,0,0,10]$$ ，状态 $$s_1$$ 的时候会得到 5 的奖励，进入第七个状态 $$s_7$$ 的时候会得到 10 的奖励，进入其他状态都没有奖励。

对4步回合（ $$\gamma=0.5$$ ）来采样回报 $$G$$ ，可通过蒙特卡洛（Monte Carlo，MC）采样的方法计算 $$s_4$$ 的价值，即
  - $$s_4,s_5,s_6,s_7$$ 的回报： $$0+0.5\times0+0.5^2\times0+0.5^3\times10=1.25$$
  - $$s_4,s_3,s_2,s_1$$ 的回报： $$0+0.5\times0+0.5^2\times0+0.5^3\times5=0.625$$

  - $$\vdots$$
  
  - $$s_4,s_5,s_6,s_6$$ 的回报： $$0+0.5\times0+0.5^2\times0+0.5^3\times0=0$$
从 $$s_4$$ 开始，采样生成很多轨迹，把这些轨迹的回报都计算出来，然后将其取平均值作为我们进入 $$s_4$$ 的价值。

### 2.2.2 贝尔曼方程

从价值函数中推导出贝尔曼方程（Bellman equation）。
#### 2.2.2.1 全期望公式

全期望公式（law of total expectation）也被称为叠期望公式（law of iterated expectations，LIE）。

$$E[V(s_{t+1})|s_t]=E[E[G_{t+1}|s_{t+1}]|s_t]=E[G_{t+1}|s_t]$$

证明过程：

$$\begin{align}
E[E[G_{t+1}|s_{t+1}]|s_t]&=E[E[g^{\prime}|s^{\prime}]|s] \nonumber\\
&=E\Bigg[\sum_{g^{\prime}} g^{\prime}p(g^{\prime}|s^{\prime})|s\Bigg]\nonumber\\
&=\sum_{s^{\prime}}\sum_{g^{\prime}} g^{\prime}p(g^{\prime}|s^{\prime},s)p(s^{\prime}|s) \nonumber\\
&=\sum_{s^{\prime}}\sum_{g^{\prime}} \frac{g^{\prime}p(g^{\prime}|s^{\prime},s)p(s^{\prime}|s)p(s)}{p(s)} \nonumber\\
&=\sum_{s^{\prime}}\sum_{g^{\prime}} \frac{g^{\prime}p(g^{\prime}|s^{\prime},s)p(s^{\prime},s)}{p(s)} \nonumber\\
&=\sum_{s^{\prime}}\sum_{g^{\prime}} \frac{g^{\prime}p(g^{\prime},s^{\prime},s)}{p(s)} \nonumber\\
&=\sum_{s^{\prime}}\sum_{g^{\prime}} g^{\prime}p(g^{\prime},s^{\prime}|s)\nonumber\\
&=\sum_{g^{\prime}}\sum_{s^{\prime}} g^{\prime}p(g^{\prime},s^{\prime}|s)\nonumber\\
&=\sum_{g^{\prime}} g^{\prime}p(g^{\prime}|s)\nonumber\\
&=E[g^{\prime}|s] \nonumber\\
&=E[G_{t+1}|s_t] \nonumber
\end{align}$$

#### 2.2.2.2 贝尔曼方程推导

贝尔曼方程就是当前状态与未来状态的迭代关系，表示当前状态的价值函数可以通过下个状态的价值函数来计算。贝尔曼方程因其提出者、动态规划创始人理查德·贝尔曼（Richard Bellman）而得名，也被叫作“动态规划方程”。
推导过程：

$$\begin{align}
V(s)&=E[G_t|s_t=s] \nonumber\\
&=E[r_{t+1}+\gamma r_{t+2}+{\gamma}^2r_{t+3}+\cdots|s_t=s] \nonumber\\
&=E[r_{t+1}|s_t=s]+\gamma E[r_{t+2}+\gamma r_{t+3}+{\gamma}^2r_{t+4}+\cdots|s_t=s] \nonumber\\
&=R(s)+\gamma E[G_{t+1}|s_t=s] \nonumber\\
&=R(s)+\gamma E[V(s_{t+1})|s_t=s] \nonumber\\
&=R(s)+\gamma \sum_{s^{\prime}\in S}p(s^{\prime}|s)V(s^{\prime}) \nonumber
\end{align}$$

把贝尔曼方程写成矩阵的形式：

$$\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_N)
\end{pmatrix}
=\begin{pmatrix}
R(s_1) \\
R(s_2) \\
\vdots \\
R(s_N)
\end{pmatrix}
+
\gamma
\begin{pmatrix}
p(s_1|s_1) & p(s_2|s_1) & \cdots & p(s_N|s_1) \\
p(s_1|s_2) & p(s_2|s_2) & \cdots & p(s_N|s_2) \\
\vdots & \vdots & & \vdots \\
p(s_1|s_N) & p(s_2|s_N) & \cdots & p(s_N|s_N) \\
\end{pmatrix}
\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_N)
\end{pmatrix}$$

求解:

$$\begin{align}
\mathbf{V}&=\mathbf{R}+\gamma \mathbf{P}\mathbf{V} \nonumber\\
\mathbf{I}\mathbf{V}&=\mathbf{R}+\gamma \mathbf{P}\mathbf{V} \nonumber\\
(\mathbf{I}-\gamma \mathbf{P})\mathbf{V}&=\mathbf{V} \nonumber\\
\mathbf{V}&=(\mathbf{I}-\gamma \mathbf{P})^{-1}\mathbf{R} \nonumber
\end{align}$$

可以通过矩阵求逆把 $$\mathbf{V}$$ 的价值直接求出来，但该矩阵求逆的过程的复杂度是 $$O(N^3)$$ 。
如果状态非常多，比如从10个状态到1000个状态，或者到100万个状态，当我们有100万个状态的时候，状态转移矩阵就会是一个100万乘100万的矩阵，对这样一个大矩阵求逆是非常困难的。所以这种通过解析解的方法只适用于很小量的马尔可夫奖励过程。

### 2.2.3 计算马尔可夫奖励过程价值的迭代算法

我们可以将迭代的方法应用于状态非常多的马尔可夫奖励过程（large MRP），比如：动态规划方法，蒙特卡洛的方法（通过采样的办法计算它），时序差分学习（temporal-difference learning，TD learning）的方法（时序差分学习是动态规划方法和蒙特卡洛方法的一个结合）。
## 2.3 马尔可夫决策过程

相对于马尔可夫奖励过程，马尔可夫决策过程多了决策（决策是指动作），其他的定义与马尔可夫奖励过程的是类似的。此外，状态转移也多了一个条件，变成了 $$p(s_{t+1}=s^{\prime}|s_t=s,a_t=a)$$ 。未来的状态不仅依赖于当前的状态，也依赖于在当前状态智能体采取的动作。马尔可夫决策过程满足条件：

$$p(s_{t+1}|s_t,a_t)=p(s_{t+1}|h_t,a_t)$$

对于奖励函数，它也多了一个当前的动作，变成了 $$R(s_t=s,a_t=a)=E[r_t|s_t=s,a_t=a]$$ 。当前的状态以及采取的动作会决定智能体在当前可能得到的奖励多少。

### 2.3.1 马尔可夫决策过程中的策略

策略定义了在某一个状态应该采取什么样的动作。知道当前状态后，我们可以把当前状态代入策略函数来得到一个概率，即

$$\pi(a|s)=p(a_t=a|s_t=s)$$

已知马尔可夫决策过程和策略 $$\pi$$ ，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程中，状态转移函数 $$p(s^{\prime}|s,a)$$ 基于它当前的状态以及它当前的动作。因为我们现在已知策略函数，也就是已知在每一个状态下，可能采取的动作的概率，所以我们就可以直接把动作进行加和，去掉 $$a$$ ，这样我们就可以得到对于马尔可夫奖励过程的转移，这里就没有动作，即

$$p_{\pi}(s^{\prime}|s)=\sum_{a\in A} \pi(a|s)p(s^{\prime}|s,a)$$

对于奖励函数，我们也可以把动作去掉，这样就会得到类似于马尔可夫奖励过程的奖励函数，即

$$R_{\pi}(s)=\sum_{a\in A} \pi(a|s)R(s,a)$$

### 2.3.2 马尔可夫决策过程和马尔可夫过程/马尔可夫奖励过程的区别

马尔可夫过程/马尔可夫奖励过程的状态转移是直接决定的。比如当前状态是 $$s$$ ，那么直接通过转移概率决定下一个状态是什么。
马尔可夫决策过程，它的中间多了一层动作 $$a$$ ，即智能体在当前状态的时候，首先要决定采取某一种动作，这样我们会到达某一个黑色的节点。到达这个黑色的节点后，因为有一定的不确定性，所以当智能体当前状态以及智能体当前采取的动作决定过后，智能体进入未来的状态其实也是一个概率分布。在当前状态与未来状态转移过程中多了一层决策性，这是马尔可夫决策过程与之前的马尔可夫过程/马尔可夫奖励过程很不同的一点。在马尔可夫决策过程中，动作是由智能体决定的，智能体会采取动作来决定未来的状态转移。

### 2.3.3 马尔可夫决策过程中的价值函数
马尔可夫决策过程中的价值函数可定义为

$$V_{\pi}(s)=E_{\pi}[G_t|s_t=s]$$

这里引入了一个 Q 函数（Q-function）。Q 函数也被称为动作价值函数（action-value function）。Q 函数定义的是在某一个状态采取某一个动作，有可能得到的回报的一个期望，即

$$Q_{\pi}(s,a)=E_{\pi}[G_t|s_t=s,a_t=a]$$

这里的期望其实也是基于策略函数的。所以需要对策略函数进行一个加和，然后得到它的价值。对 Q 函数中的动作进行加和，就可以得到价值函数：

$$V_{\pi}(s)=\sum_{a\in A} {\pi}(a|s)Q_{\pi}(s,a)$$

此处我们对Q函数的贝尔曼方程进行推导：

$$\begin{align}
Q(s,a)&=E[G_t|s_t=s,a_t=a] \nonumber\\
&=E[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots|s_t=s,a_t=a] \nonumber\\
&=E[r_{t+1}|s_t=s,a_t=a]+\gamma E[r_{t+2}+\gamma r_{t+3}+\gamma^2r_{t+4}+\cdots|s_t=s,a_t=a] \nonumber\\
&=R(s,a)+\gamma E[G_{t+1}|s_t=s,a_t=a] \nonumber\\
&=R(s,a)+\gamma E[V(s_{t+1})|s_t=s,a_t=a] \nonumber\\
&=R(s,a)+\gamma \sum_{s^{\prime}\in S} p(s^{\prime}|s,a)V(s^{\prime}) \nonumber
\end{align}$$

### 2.3.4 贝尔曼期望方程

我们可以把状态价值函数和 Q 函数拆解成两个部分：即时奖励和后续状态的折扣价值（discounted value of successor state）。通过对状态价值函数进行分解，我们就可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程——贝尔曼期望方程（Bellman expectation equation）：

$$V_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma V_{\pi}(s_{t+1})|s_t=s]$$

类似的，Q函数的贝尔曼期望方程：

$$Q_{\pi}(s,a)=E_{\pi}[r_{t+1}+\gamma Q_{\pi}(s_{t+1},a_{t+1})|s_t=s,a_t=a]$$

将 $$Q(s,a)=R(s,a)+\gamma \sum_{s^{\prime}\in S} p(s^{\prime}|s,a)V(s^{\prime}) $$ 带入 $$V_{\pi}(s)=\sum_{a\in A} {\pi}(a|s)Q_{\pi}(s,a)$$ 得到当前状态价值与未来状态价值之间的关联：

$$V_{\pi}(s)=\sum_{a \in A} \pi(a|s) \bigg(R(s,a)+\gamma \sum_{s^{\prime}\in S} p(s^{\prime}|s,a)V(s^{\prime})\bigg)$$

将 $$V_{\pi}(s)=\sum_{a\in A} {\pi}(a|s)Q_{\pi}(s,a)$$ 带入 $$Q(s,a)=R(s,a)+\gamma \sum_{s^{\prime}\in S} p(s^{\prime}|s,a)V(s^{\prime}) $$ 得到当前时刻Q函数与未来时刻Q函数之间的关联：

$$Q_{\pi}(s,a)=R(s,a)+\gamma \sum_{s^{\prime} \in S} p(s^{\prime}|s,a)\sum_{a^{\prime} \in A} \pi(a^{\prime}|s^{\prime})Q_{\pi}(s^{\prime},a^{\prime})$$

### 2.3.5 备份图
备份（backup）类似于自举之间的迭代关系，对于某一个状态，它的当前价值是与它的未来价值线性相关的。我们将与下图类似的图称为备份图（backup diagram），因为它们所示的关系构成了更新或备份操作的基础，而这些操作是强化学习方法的核心。这些操作将价值信息从一个状态（或状态-动作对）的后继状态（或状态-动作对）转移回它。每一个空心圆圈代表一个状态，每一个实心圆圈代表一个状态-动作对。

式 $$V_{\pi}(s)=\sum_{a \in A} \pi(a|s) \bigg(R(s,a)+\gamma \sum_{s^{\prime}\in S} p(s^{\prime}|s,a)V(s^{\prime})\bigg)$$ ，这里有两层加和。第一层加和是对叶子节点进行加和，往上备份一层，我们就可以把未来的价值（ $$s^{\prime}$$ 的价值）备份到黑色的节点。第二层加和是对动作进行加和，黑色节点的价值再往上备份一层，就会得到根节点的价值，即当前状态的价值。
